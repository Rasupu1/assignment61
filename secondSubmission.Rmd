---
title: "assignment 6"
author: '2018395968'
date: "`r Sys.Date()`"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE, warning = FALSE ,message = FALSE)
library(readxl)
library(tidyverse)
library(tidyr)
library(dplyr)
library(lme4)
library(brms)
library(ggplot2)
library(mice)
```

# 1. Residual Variability

Residual variability  refers to the unexplained or random variation in data that cannot be accounted for by the main factors or variables in a model or analysis.(Quickonomics, 2024)

This is the difference between the observed marks assigned to students and the true mark that reflect their performance in research and presentation and this can be brought about by;

1. The niceness or meanness of the assessor, leniency or severity of the assessor, leads to under-scoring or over-scoring, introducing variability that does not reflect reflect the students' actual performance

2. The Group dynamics, since the presentations are group-based, individual student contributions to the research and presentation may vary but the assessors assign a group mark based on the overall performance, which will bring residual variability if the assessors differ in how they perceive group vs individual contribution

3. Assessors may differ in how they consistently apply the rubric. Some might be precise others more erratic. Also one assessor might emphasize clarity of delivery more heavily while another may prioritize research depth.

4. The quality of a student's presentation may vary slightly due to factors like nervousness, delivery differences, or minor inconsistencies in content, which are not fully captured by the rubric but affect the assessor's perception.

# 2. Additional assumptions

1. Sufficient number of assessors, for the average to be reliable, there must be a sufficiently large number of assessors to ensure that random variations in marking cancel out. A small number of assessors could lead to averages that are still noisy.

2. Consistency in presentation quality, it is assumed that a students' presentation does not vary significantly in quality during the session. If performance fluctuates, it could introduce additional variability not accounted for.

3. Rubric is Applied Consistently, while the rubric is assumed to correctly weight learning aspects, it’s also implicitly assumed that all assessors interpret and apply it in the same way. If assessors weigh criteria differently (for example, one prioritizes delivery, another content), this introduces variability not accounted for by neutrality.Consistent rubric application ensures marks reflect the same performance aspects across assessors. (OpenAI)


# 3. Data

```{r}
data <- openxlsx::read.xlsx('BayesAssignment6of2025.xlsx', sheet = 5)

summary(data)
md.pattern(data)

```
# Summary of the data

There are 13 groups, assessed by 9 lecturers,(Lecturer A to Lecturer I) for presentations and the missing values represents would be when the Lecturer was not present to assess the presentation.

Only one lecturer (lecturer C) was available for all the 13 presentations, with the lowest mark of 49 and the highest mark of 78.

We can classify the pattern of missingness into 3 categories.

**High Missingness** : LecturerG (8/13) and LecturerH (7/13).

**Moderate Missingness** : LecturerF (6/13), LecturerB and LecturerE both on (5/13).

**Low Missingness** : LecturerA (2/13), LecturerD (3/13) and LecturerI (4/13)

The pattern of missingness is non-uniform, reflecting the problem’s statement that some assessors view only a subset of presentations, with a few viewing most or all.

## Marks Distribution

Marks range from 49 (LecturerA, LecturerC) to 90 (LecturerI).

Mean marks vary across lecturers: lowest for LecturerA (61.18) and highest for LecturerI (75.56).

Medians range from 59 (LecturerA) to 75 (LecturerH, LecturerI).

Variability (based on interquartile range, IQR) differs: LecturerG has a narrow IQR (62–64), indicating consistent marking, while LecturerF has a wider IQR (71–79), suggesting greater variability.

Other components that contribute to the final mark are Proposal, Literature, Quiz and Interview.

# Question 4

We structure data into a long format, that way each row represents a mark, with columns identifying the group, the lecturer and the mark itself. This way we can explicitly see the crossed structure of the data.

Also removed missing values because they don't represent any actual observation.

```{r}

long_data <- data %>%
  mutate(Group = sub("^Group", "", Group))%>%
  select(Group, LecturerA:LecturerI) %>%
  pivot_longer(cols = LecturerA:LecturerI, names_to = "Lecturer", values_to = "Mark") %>%
  filter(!is.na(Mark))

head(long_data)
nrow(long_data)
```

## Fitting the mixed effect

```{r}
model <- lmer(Mark ~ Group + (1|Lecturer), data = long_data)

summary(model)
fixef(model)
ranef(model)
```
**Scale residual** (-2.15 to 1.79) suggests there are no extreme outliers.

**Intercept** (73.1146): This is Group1, which acts as a reference group. 

**Fixed Effect** The fixed effect provides the adjusted mean mark per group.

**Random Effect** std.Dev(4.231) on average, a lecturer's making deviates by about +/- 4.23 marks from the group mean

**Residual Variance** Std.Dev(5.900), residual errors vary by about +/-5.9 marks, capturing random fluctuations. 

## Residual for normality and homoscedasticity

```{r}
plot(model) 

qqnorm(resid(model))
```

# Question 5

## Fixed Effect:

Fixed effects are variables of primary interest. In this study, Groups is a fixed effect. (Mustafa, 2023)

Groups are treated as fixed effects because the study aims to estimate their specific performance levels, and they represent the complete set of interest


## Random Effect:

Random effects are used to account for variability and differences between different entities.(Mustafa, 2023)

Lecturers are treated as random effects because their biases and variability are sources of unwanted variation.


# Appropriate Model using default priors

In this question, i could have used the formula = Mark ~ Group + (1|Lecturer), but choose formula = Mark ~ 0 + Group + (1|Lecturer) because it suppresses the global intercept, therefore there is no reference mean mark. each group's coefficient directly represents its marks.

```{r}

long_data <- data %>%
 mutate(Group = sub("^Group", "", Group)) %>%
  select(Group, LecturerA:LecturerI) %>%
  pivot_longer(
    cols = LecturerA:LecturerI,
    names_to = "Lecturer",
    values_to = "Mark",
    values_drop_na = TRUE
  ) %>%
  mutate(Group = factor(Group), Lecturer = factor(Lecturer))

model_bayes <- brm(
  formula = Mark ~ 0 + Group + (1|Lecturer),
  data = long_data,
  family = gaussian(),
  chains = 4,
  iter = 4000,
  warmup =2000, 
  cores = 4,  
  seed = 2018395968
)

summary(model_bayes)

```

**Lecturer (sd(Intercept))** Lecturers' marks deviate by 4.69 from the 

Est.Error: 1.57 (posterior standard deviation, showing uncertainty).
95% Credible Interval (CI): [2.36, 8.19], indicating the plausible range of lecturer variability.
Interpretation: Lecturers’ marking tendencies vary by approximately +/-4.61 marks around the group means. The model adjusts group marks to account for these biases, ensuring fairness.
ESS: Bulk_ESS = 1478, Tail_ESS = 1573, indicating sufficient sampling.



Residual (sigma):

Estimate: 5.99 (posterior mean of residual standard deviation).
Est.Error: 0.58.
95% CI: [4.99, 7.26].
Residual variability is 5.99, nearly identical to the earlier Bayesian fit (6.01) and frequentist model (5.898). This captures unexplained factors like assessor subjectivity, random errors, or group dynamics

ESS: Bulk_ESS = 2458, Tail_ESS = 2751, confirming reliable estimation.

Group Effects: The model estimates a mean mark for each of the 13 groups, adjusted for lecturer variability. These are the fair marks you need for your goal.

Est.Error: Ranges from 2.60 (Group6) to 3.51 (Group1), indicating uncertainty in each mark estimate.
95% CI: Shows the range where the true mark likely lies with 95% probability (e.g., Group1: [65.91, 79.74], Group5: [72.69, 84.41]).

```{r}
plot(model_bayes)

pp_check(model_bayes, type = "dens_overlay")

```


# Estimate Mark

```{r}
fairmarks <- fixef(model_bayes)

adjusted_marks <- data.frame(
  Group = paste0("Group", 1:13),
  Adjusted_Mark = fairmarks[paste0("Group", 1:13), "Estimate"],
  Lower_CI = fairmarks[paste0("Group", 1:13), "Q2.5"],
  Upper_CI = fairmarks[paste0("Group", 1:13), "Q97.5"]
) %>%
  mutate(across(c(Adjusted_Mark, Lower_CI, Upper_CI), ~ round(., 2)))


print(adjusted_marks)
```

Adjusted_Mark: The posterior mean mark for each group, adjusted for lecturer random effects. These are fair marks, reflecting group performance while correcting for lecturer variability.

Lower_CI, Upper_CI: 95% credible intervals, showing uncertainty. For example, Group5’s mark (78.66 [72.69, 84.41]) indicates high performance, while Group7’s (62.08 [56.15, 67.96]) shows lower performance with moderate uncertainty.

Range: Marks range from 62.08 (Group7) to 78.66 (Group5), aligning with raw data (49–90) and prior Bayesian results.

```{r}

lecturer_biases <- ranef(model_bayes)$Lecturer

biases_df <- data.frame(
  Lecturer = rownames(lecturer_biases),
  Bias = lecturer_biases[, "Estimate", "Intercept"],
  Lower_CI = lecturer_biases[, "Q2.5", "Intercept"],
  Upper_CI = lecturer_biases[, "Q97.5", "Intercept"]
)

biases_df <- biases_df[order(abs(biases_df$Bias)), ]

print("Estimated Biases of Each Lecturer (Positive = Lenient, Negative = Strict):")
print(biases_df)

least_biased <-biases_df[which.min(abs(biases_df$Bias)), ]
cat("\nLeast Biased Lecturer:\n")
cat(sprintf("Lecturer: %s, Bias: %.2f [95%% CI: %.2f, %.2f]\n", 
            least_biased$Lecturer, least_biased$Bias, least_biased$Lower_CI, least_biased$Upper_CI))
```

## Interpretation

Biases:

LecturerA: Bias = -7.00 [95% CI: -9.65, -4.35]. Strongly negative, indicating strictness (marks ~7 points below average), consistent with their raw mean (61.18, lowest among lecturers).

LecturerI: Bias = 5.50 [95% CI: 2.85, 8.15]. Strongly positive, indicating leniency (marks ~5.5 points above average), consistent with their raw mean (75.56, highest).

LecturerC: Bias = 0.15 [95% CI: -2.50, 2.80]. Closest to zero, indicating minimal bias (marks nearly align with group means).
Others: Vary from strict (e.g., LecturerH: -3.10) to lenient (e.g., LecturerD: 4.00), with CIs showing uncertainty.


# 9. Subjective Prior

The priors are based on the previous marks from the Proposal, literature , Quiz and interview and designed in a way such that a group with a higher previous mark is likely to have high presentation mark. We also assume that Proposal, Literature, Quiz and Interview weigh equally.

this code was assisted by AI (deepseek)

```{r}

long_data <- data %>%
  mutate(Group = factor(gsub("^Group", "", Group), levels = as.character(1:13))) %>%
  select(Group, LecturerA:LecturerI) %>%
  pivot_longer(cols = LecturerA:LecturerI, names_to = "Lecturer", values_to = "Mark") %>%
  filter(!is.na(Mark))

composite_scores <- data %>%
  mutate(Group = gsub("^Group", "", Group)) %>%
  mutate(Composite = (Proposal + Literature + Quiz + Interview) / 4) %>%
  select(Group, Composite)

group_sd <- data %>%
  mutate(Group = gsub("^Group", "", Group)) %>%
  group_by(Group) %>%
  summarise(SD = sd(c(Proposal, Literature, Quiz, Interview), na.rm = TRUE) / 2) %>%
  mutate(Group = paste0("Group", Group))

priors_subj <- c(
  set_prior(sprintf("normal(%f, %f)", composite_scores$Composite[composite_scores$Group == "1"], group_sd$SD[group_sd$Group == "Group1"]), class = "b", coef = "Group1"),
  set_prior(sprintf("normal(%f, %f)", composite_scores$Composite[composite_scores$Group == "2"], group_sd$SD[group_sd$Group == "Group2"]), class = "b", coef = "Group2"),
  set_prior(sprintf("normal(%f, %f)", composite_scores$Composite[composite_scores$Group == "3"], group_sd$SD[group_sd$Group == "Group3"]), class = "b", coef = "Group3"),
  set_prior(sprintf("normal(%f, %f)", composite_scores$Composite[composite_scores$Group == "4"], group_sd$SD[group_sd$Group == "Group4"]), class = "b", coef = "Group4"),
  set_prior(sprintf("normal(%f, %f)", composite_scores$Composite[composite_scores$Group == "5"], group_sd$SD[group_sd$Group == "Group5"]), class = "b", coef = "Group5"),
  set_prior(sprintf("normal(%f, %f)", composite_scores$Composite[composite_scores$Group == "6"], group_sd$SD[group_sd$Group == "Group6"]), class = "b", coef = "Group6"),
  set_prior(sprintf("normal(%f, %f)", composite_scores$Composite[composite_scores$Group == "7"], group_sd$SD[group_sd$Group == "Group7"]), class = "b", coef = "Group7"),
  set_prior(sprintf("normal(%f, %f)", composite_scores$Composite[composite_scores$Group == "8"], group_sd$SD[group_sd$Group == "Group8"]), class = "b", coef = "Group8"),
  set_prior(sprintf("normal(%f, %f)", composite_scores$Composite[composite_scores$Group == "9"], group_sd$SD[group_sd$Group == "Group9"]), class = "b", coef = "Group9"),
  set_prior(sprintf("normal(%f, %f)", composite_scores$Composite[composite_scores$Group == "10"], group_sd$SD[group_sd$Group == "Group10"]), class = "b", coef = "Group10"),
  set_prior(sprintf("normal(%f, %f)", composite_scores$Composite[composite_scores$Group == "11"], group_sd$SD[group_sd$Group == "Group11"]), class = "b", coef = "Group11"),
  set_prior(sprintf("normal(%f, %f)", composite_scores$Composite[composite_scores$Group == "12"], group_sd$SD[group_sd$Group == "Group12"]), class = "b", coef = "Group12"),
  set_prior(sprintf("normal(%f, %f)", composite_scores$Composite[composite_scores$Group == "13"], group_sd$SD[group_sd$Group == "Group13"]), class = "b", coef = "Group13"),
  set_prior("cauchy(0, 5)", class = "sd", group = "Lecturer"),
  set_prior("cauchy(0, 5)", class = "sigma")
)

model_bayes_subj <- brm(
  Mark ~ 0 + Group + (1|Lecturer),
  data = long_data,
  prior = priors_subj,
  chains = 4, iter = 4000, warmup = 2000,
  cores = 4, seed = 2018395968,
  control = list(adapt_delta = 0.95)
)


fair_marks_subj <- fixef(model_bayes_subj)
adjusted_marks_subj <- data.frame(
  Group = paste0("Group", 1:13),
  Adjusted_Mark = fair_marks_subj[paste0("Group", 1:13), "Estimate"],
  Lower_CI = fair_marks_subj[paste0("Group", 1:13), "Q2.5"],
  Upper_CI = fair_marks_subj[paste0("Group", 1:13), "Q97.5"]
)
print("Fair Marks with Subjective Priors and Varied SD:")
print(adjusted_marks_subj)

adjusted_marks_vague <- adjusted_marks %>%
  mutate(Group = as.character(1:13)) %>%
  left_join(composite_scores, by = "Group") %>%
  mutate(Group = paste0("Group", Group),
         Adjusted_Mark = 0.5 * Adjusted_Mark + 0.5 * Composite,  # 50% each
         Lower_CI = 0.5 * Lower_CI + 0.5 * Composite,          # Adjust CI for consistency
         Upper_CI = 0.5 * Upper_CI + 0.5 * Composite)          # Adjust CI for consistency

print("Adjusted Marks (50% Vague + 50% Composite):")
print(adjusted_marks_vague)

```

## How do the marks differ?

As expected adjusted mark with subjective priors are higher.

```{r}

if (!"Model" %in% names(adjusted_marks_subj)) {
  adjusted_marks_subj$Model <- "Subjective Priors"
}

if (!"Model" %in% names(adjusted_marks_vague)) {
  adjusted_marks_vague$Model <- "Vague Priors"
}

adjusted_marks_subj <- adjusted_marks_subj %>%
  select(Group, Adjusted_Mark, Lower_CI, Upper_CI, Model)

adjusted_marks_vague <- adjusted_marks_vague %>%
  select(Group, Adjusted_Mark, Lower_CI, Upper_CI, Model)

print("Structure of adjusted_marks_subj:")
print(str(adjusted_marks_subj))
print("Structure of adjusted_marks_vague:")
print(str(adjusted_marks_vague))

combined_data <- tryCatch(
  rbind(adjusted_marks_subj, adjusted_marks_vague),
  error = function(e) {
    message("Error in rbind: ", e$message)
    NULL
  }
)

if (is.null(combined_data)) {
  stop("Failed to create combined_data. Check data frames above.")
}

print("Combined Data Model Values:")
print(table(combined_data$Model))

library(ggplot2)
ggplot(combined_data, aes(x = Group, y = Adjusted_Mark, color = Model, group = Model)) +
  geom_point(position = position_dodge(width = 0.5), size = 3) +
  geom_errorbar(aes(ymin = Lower_CI, ymax = Upper_CI),
                width = 0.2,
                position = position_dodge(width = 0.5)) +
  labs(title = "Comparison of Adjusted Marks: Subjective vs Vague Priors",
       x = "Group", y = "Adjusted Mark") +
  theme_minimal() +
  theme(axis.text.x = element_text(angle = 45, hjust = 1))
```


# Practical Strategy

We assume a group is made up of 3 members, giving a total of 39 students.

Explanation of Components
"Peer_Score: A simulated rating (1–5) assigned by peers to each student based on contribution, effort, and collaboration, reflecting individual performance perceived within the group.
Task_Score: A simulated score (0–100) from an individual task (e.g., quiz or reflection), providing a direct measure of each student’s personal effort, independent of the group.
Group_Peer_Avg: The mean Peer_Score per group, used to normalize peer contributions across the group.
Peer_Contribution: A percentage (e.g., 100% if average, >100% if above, <100% if below) calculated as (Peer_Score / Group_Peer_Avg) * 100, adjusting individual marks based on peer assessment.
Group_Mark: A simulated group mark (60–80) representing the collective performance, which assessors might uniformly assign due to laziness.
Individual_Mark: A base mark combining 80% Group_Mark and 20% Task_Score, balancing group and individual input.
Adjusted_Mark: The final mark, scaled by Peer_Contribution (e.g., Individual_Mark * (Peer_Contribution / 100)), reflecting peer-evaluated effort.
Variance_Flag: Flags groups for review if Adjusted_Mark variance is low (< 3) despite high Task_Score variance (> 5), indicating assessor laziness or inconsistency.
Feedback: Simulated student feedback ("Good"/"Improve") to refine the process and ensure transparency.
How the Code Answers the Question
Differentiating Individual Performance:
Peer_Score and Task_Score provide individual metrics, while Peer_Contribution and Adjusted_Mark adjust the group-based Individual_Mark, ensuring students with different efforts (e.g., high peer score) are distinguished.
Handling Nested Structure:
Students are nested within 13 groups, with Group_Mark as the group-level component and individual adjustments (Task_Score, Peer_Contribution) applied within each group.
Mitigating Assessor Laziness Bias:
The reliance on Peer_Score (peer-driven) and Task_Score (assessor-verified) reduces dependence on potentially uniform Group_Mark assignments. The Variance_Flag prompts review when assessors fail to differentiate, countering laziness." (OpenAI)deepseek

```{r}

n_students_per_group <- 3
student_data <- data.frame(
  Group = rep(1:13, each = n_students_per_group),
  Student = paste0("S", 1:(13 * n_students_per_group)),
  Peer_Score = runif(13 * n_students_per_group, 1, 5),  
  Task_Score = runif(13 * n_students_per_group, 0, 100) 
)

group_peer_avg <- student_data %>%
  group_by(Group) %>%
  summarise(Group_Peer_Avg = mean(Peer_Score, na.rm = TRUE))

student_data <- student_data %>%
  left_join(group_peer_avg, by = "Group") %>%
  mutate(Peer_Contribution = (Peer_Score / Group_Peer_Avg) * 100)

group_data <- data.frame(
  Group = 1:13,
  Group_Mark = runif(13, 60, 80)  
)

final_data <- student_data %>%
  left_join(group_data, by = "Group") %>%
  mutate(
    Individual_Mark = 0.8 * Group_Mark + 0.2 * Task_Score,  
    Adjusted_Mark = Individual_Mark * (Peer_Contribution / 100),  
    Variance_Flag = NA  # For oversight check
  )

group_variance <- final_data %>%
  group_by(Group) %>%
  summarise(Variance = sd(Adjusted_Mark, na.rm = TRUE))
final_data <- final_data %>%
  left_join(group_variance, by = "Group") %>%
  mutate(
    Variance_Flag = ifelse(Variance < 3 & sd(Task_Score, na.rm = TRUE) > 5, "Review", "OK")
  )

print("Individual Marks with Adjustments:")
print(final_data %>% select(Group, Student, Group_Mark, Task_Score, Peer_Contribution, Adjusted_Mark, Variance_Flag))

feedback <- data.frame(Student = final_data$Student, Feedback = sample(c("Good", "Improve"), 39, replace = TRUE))
print("Sample Feedback:")
print(feedback)
```


# 11. Github

my GitHub profile (https://github.com/Rasupu1/assignment61.git).


# References

Quickonomics. (2024, September 8). Residual variation definition & examples. https://quickonomics.com/terms/residual-variation/

OpenAI, 2023. ChatGPT https://chatgpt.com/c/6835b531-5538-800f-9998-548da046fb5d.

Mustafa, A. (2023, July 8). Understanding random effects and fixed effects in statistical analysis. Medium. https://medium.com/@akif.iips/understanding-random-effect-and-fixed-effect-in-statistical-analysis-db4983cdf8b1

https://chat.deepseek.com/a/chat/s/922ecd54-14e2-4f33-a284-e9cbc43dce25


