---
title: "assignment 6"
author: '2018395968'
date: "`r Sys.Date()`"
output: word_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)
library(readxl)
library(tidyverse)
library(tidyr)
library(dplyr)
library(lme4)
library(brms)


```

# 1. Residual Variability

This is the difference between the observed marks assigned to students and the true mark that reflect their performance in research and presentation and this can be brought about by;

1. The niceness or meanness of the assessor, leniency or severity of the assessor, leads to under-scoring or over-scoring, introducing variability that does not reflect reflect the students' actual performance

2. The Group dynamics, since the presentations are group-based, individual student contributions to the research and presentation may vary but the assessors assign a group mark based on the overall performance, which will bring residual variability if the assessors differ in how they perceive group vs individual contribution

3. Assessors may differ in how they consistently apply the rubric. Some might be precise others more erratic. Also one assessor might emphasize clarity of delivery more heavily while another may prioritize research depth.

4. The quality of a student's presentation may vary slightly due to factors like nervousness, delivery differences, or minor inconsistencies in content, which are not fully captured by the rubric but affect the assessor's perception.

# 2.Additional assumptions

1. Sufficient number of assessors, for the average to be reliable, there must be a sufficiently large number of assessors to ensure that random variations in marking cancel out. A small number of assessors could lead to averages that are still noisy.

2. Consistency in presentation quality, it is assumed that a students' presentation does not vary significantly in quality during the session. If performance fluctuates, it could introduce additional variability not accounted for.

3. Rubric application consistency, while the rubric is assumed to be correctly weighted, it’s also implicitly assumed that assessors apply it consistently enough that their marks reflect the same underlying performance criteria. Significant deviations in how assessors interpret the rubric could undermine the average’s accuracy.


# Question 3

```{r}
data <- openxlsx::read.xlsx('BayesAssignment6of2025.xlsx', sheet = 5)

summary(data)
md.pattern(data)

```
# Summary

There are 13 groups, assessed by 9 lecturers,(Lecturer A to Lecturer I) for presentations and the missing values represents would be when the Lecturer was not present to assess the presentation.

Only one lecturer (lecturer C) was available for all the 13 presentations, with the lowest mark of 49 and the highest mark of 78.

We can classify the pattern of missingness into 3 categories.

*High Missingness* : LecturerG (8/13) and LecturerH (7/13).

*Moderate Missingness* : LecturerF (6/13), LecturerB and LecturerE both on (5/13).

*Low Missingness* : LecturerA (2/13), LecturerD (3/13) and LecturerI (4/13)

The pattern of missingness is non-uniform, reflecting the problem’s statement that some assessors view only a subset of presentations, with a few viewing most or all.

## Marks Distribution

Marks range from 49 (LecturerA, LecturerC) to 90 (LecturerI).

Mean marks vary across lecturers: lowest for LecturerA (61.18) and highest for LecturerI (75.56).

Medians range from 59 (LecturerA) to 75 (LecturerH, LecturerI).

Variability (based on interquartile range, IQR) differs: LecturerG has a narrow IQR (62–64), indicating consistent marking, while LecturerF has a wider IQR (71–79), suggesting greater variability.

Other components that contribute to the final mark are Proposal, Literature, Quiz and Interview.

# Question 4

We structure data into a long format, that way each row represents a mark, with columns identifying the group, the lecturer and the mark itself. This way we can explicitly see the crossed structure of the data.

Also removed missing values because they dont represent any actual observation.

```{r}

long_data <- data %>%
  select(Group, LecturerA:LecturerI) %>%
  pivot_longer(cols = LecturerA:LecturerI, names_to = "Lecturer", values_to = "Mark") %>%
  filter(!is.na(Mark))


head(long_data)
nrow(long_data)
```

## Fitting the mixed effect

```{r}

lmer(Mark ~ 1 + (1|Group) + (1|Lecturer), data = long_data)

lmer(Mark ~ 1 + (1|Group) + (1|Lecturer), data = long_data)

#lmer(Mark ~ Group + (1|Lecturer), data = long_data)


```
REML criterion at convergence: 519.1136: This is the REML deviance, a measure of model fit. Lower values indicate better fit.

## Overall Mean Mark:

The fixed intercept (70.29) represents the average mark across all groups and lecturers, adjusted for their respective random effects. This is a baseline for what a  group might score if assessed by a  lecturer.

## Group Variability:

The group random effect standard deviation (4.238) indicates that group performance varies moderately. This helps estimate each group’s true performance, adjusted for which lecturers assessed them.

## Lecturer Variability:

The lecturer random effect standard deviation (4.164) shows that lecturers differ in their marking tendencies.This adjusts for the niceness or meanness personality, ensuring marks are not unfairly influenced by which lecturers assessed a group.

## Residual Variability:

The residual standard deviation (5.898) is the largest source of variability, indicating that factors beyond group performance and lecturer tendencies. 

# Question 5

## Fixed Effect:

Fixed effects are variables of primary interest. In this study, Groups is a fixed effect. 

Groups are treated as fixed effects because the study aims to estimate their specific performance levels, and they represent the complete set of interest


## Random Effect:

Lecturers are treated as random effects because their biases and variability are sources of unwanted variation.


